{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt')\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt')\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT\n",
    "Let's see inside of the data. train_data is list of char index.\n",
    "$$\n",
    "    \\forall i \\ \\ \\texttt{train_data[i]} \\in Z^{|s|}\n",
    "$$\n",
    "where $s$ is a sequence of charactors. And\n",
    "$$\n",
    "    s \\in \\{c \\in C\\}\n",
    "$$\n",
    "$$\n",
    "    C \\in \\{a,b,c,...,` `,`\\{`, `\\}`\\}\n",
    "$$\n",
    "where \"{\" and \"}\" are special tags for head and end of sentences.\n",
    "\n",
    "( NOTE ) we do NOT use pos-tag or any language information on PTB except sequence of charactors at all! \n",
    "and <font color=\"red\"> you might realize that the word $\"<UNK>\"$ makes strange charactor sequecne $\"<,U,N,K,>\"$. Let's think there is a word with $\"<UNK>\"$ in this world!  lol\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-2-4476b03d87d4>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-4476b03d87d4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print \"Charactor mapping : \"\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "#===========================\n",
    "#          HINT\n",
    "#===========================\n",
    "#Charactor mapings\n",
    "chars = [None]*len(utils.vocab.keys())\n",
    "for k in utils.vocab.keys(): chars[utils.vocab[k]]=k\n",
    "print \"Charactor mapping : \"\n",
    "print chars\n",
    "\n",
    "#inside of data\n",
    "samples = train_data[1000:1005]\n",
    "print \"samples : \"\n",
    "for i,s in enumerate(samples):\n",
    "    print \"indeces : \",s\n",
    "    print \"real chars : \",\n",
    "    for c in s:\n",
    "        print chars[c],\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT\n",
    "## Probabilities of LSTM model\n",
    "Here let's see what we want to code. We need LSTM which can predict next charactor from the previous sequece, i.e.,\n",
    "$$\n",
    "    {\\rm LSTM} : C^{t-1} \\rightarrow C\n",
    "$$\n",
    "thus LSTM gives probability distribution over $C$,\n",
    "$$\n",
    "    P_{{\\rm LSTM}}( s_t | s_{<t} ) = softmax(V_{[s_t]}h_t)   \n",
    "$$\n",
    "where $V_{[s_t]}$ is another parameter of vector for word $s_t$, $h_t$ is $t$-th output from the LSTM, $s_i$ is $i$-th charactor in the sentence $s$ and $s_{<t}$ is charactors before $t$.\n",
    "\n",
    "Then log likelihood of a sentence is \n",
    "$$\n",
    "    \\log \\mathcal{L} = \\sum_{0<=t<T} \\log P_{{\\rm LSTM}}(s_t| s_{<t})\n",
    "$$\n",
    "<font color=\"red\"> NOTE) we also want to predict the first charactor of a sentence which is always \"{\" and the last one which is \"}\".</font>\n",
    "\n",
    "\n",
    "## Implementation tricks\n",
    "We train the LSTM with dynamically constructing RNN sequence for each batch (we do not stick to a statistic computational graph anymore), i.e.,\n",
    "\n",
    "FOR each batch, $b$ :  \n",
    "| ${\\tt loss, score = buildModel()}$  \n",
    "| ${\\tt edf.Forward()}$  \n",
    "| ${\\tt edf.Backward(loss)}$  \n",
    "| ${\\tt edf.GradClip(10)}$  \n",
    "| ${\\tt edf.SGD(eta)}$  \n",
    "END\n",
    "\n",
    "Now ${\\tt buildModel()}$ is\n",
    "\n",
    "${\\tt buildModel()}$  \n",
    "| Global_var : ${\\tt inp}$, ${\\tt C2V}$, ${\\tt hidden\\_dim}$, $LSTM\\_PARAS$   \n",
    "| ${\\tt loss\\_list = []}$  #List of logL at each time $t$   \n",
    "| ${\\tt score= []}$       #List of probabilities at each time $t$   \n",
    "| $\\tt FOR\\ t<T$ :  \n",
    "| | chain LSTM_CELL  \n",
    "| | compute logLoss,$\\tt l$, and prob,$\\tt p$, at time $\\tt t$  \n",
    "| | ${\\tt loss\\_list += [l]}$  \n",
    "| | ${\\tt score += [p]}$   \n",
    "| $\\tt END$   \n",
    "| ${\\tt loss = average(loss\\_list)}$   \n",
    "| ${\\tt return\\ loss, score}$   \n",
    "$\\tt END$  \n",
    "\n",
    "Again $\\tt score$ is a list of probabilities on each time step $t$ and $\\tt loss$ is a log likelihood of the batch.\n",
    "\n",
    "\n",
    "## Other things you might concern \n",
    "### computational graph\n",
    "Now computational graph is not static, we need to initialize everytime.\n",
    "\n",
    "### initial state of LSTM cell \n",
    "We can think several reasonable implementation but we employed just zero vector.\n",
    "\n",
    "### masks and paddings\n",
    "We train model with mini-batch but LSTM chain has a fix size for all sentences in a batch. We need to remove output from padded part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "#debug\n",
    "batch=4\n",
    "hidden_dim=10\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "\n",
    "# LSTM parameters\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "# for sake of saving\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "# LSTM cell\n",
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "# build the model given the input inp, it shoudl return loss and prob\n",
    "def BuildModel():\n",
    "    '''\n",
    "    Global vars:\n",
    "        inp : Z^{batch_size x seq_size}\n",
    "    '''\n",
    "    \n",
    "    #initialize CP\n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0] #batch_size\n",
    "    T = inp.value.shape[1] #length of sequence (max len(s))\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) #initial state of the cell\n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) #word embedings on time t R^{batch_size x hidden_dim}\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])         #!!UNNECESSARY RESHAPE!!\n",
    "        h_next, c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        l = edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))) #R^{batch_size}\n",
    "        logloss = edf.Reshape(l, (B, 1))   #R^{batch_size x 1}\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss) #R^{batch_size x T}\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score,[l, logloss]\n",
    "    \n",
    "    \n",
    "# calculate the perplexity         \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "\n",
    "# predict the sequence\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score,_ = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    print(ep,\"-th epoch\")\n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score,debug = BuildModel()\n",
    "        edf.Forward()\n",
    "        for v in debug: print(v.value)\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_GRU.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "# GRU parameters\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "Wz = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "Wr = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "W = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "# for sake of saving\n",
    "parameters.extend([C2V, Wz, Wr, W, V])\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "# GRU cell                \n",
    "def GRUCell(xt, h):\n",
    "    \n",
    "    z = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wz))\n",
    "    r = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wr))\n",
    "    h_hat = edf.Tanh(edf.VDot(edf.ConCat(xt, edf.Mul(r, h)), W))\n",
    "    h_next = edf.Add(edf.Mul(z, h_hat), edf.Mul(h, edf.Add(edf.Value(1), edf.Mul(z, edf.Value(-1)))))\n",
    "    \n",
    "    return h_next\n",
    "\n",
    "\n",
    "# build the model given the input inp, it shoudl return loss and prob\n",
    "def BuildModel():\n",
    "    '''\n",
    "    Global vars:\n",
    "        inp : Z^{batch_size x seq_size}\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #initialize CP\n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0] #batch_size\n",
    "    T = inp.value.shape[1] #length of sequence (max len(s))\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) #initial state of the cell\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next \n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "\n",
    "# calculate the perplexity     \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "\n",
    "# predict the sequence\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "           \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx  \n",
    "\n",
    "    \n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
