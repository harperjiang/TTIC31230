{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load('./c10_data/train.npz')\n",
    "t_imgs = np.float32(data['imgs'])/255.\n",
    "\n",
    "# Reshape the train image data to (idx, h, w, channel)\n",
    "t_imgs = t_imgs.reshape(50000, 32, 32, 3)\n",
    "t_labels = np.float32(data['labels'])\n",
    "\n",
    "data = np.load('./c10_data/test.npz')\n",
    "v_imgs = np.float32(data['imgs'])/255.\n",
    "\n",
    "# Reshape the valid image data to (idx, h, w, channel)\n",
    "v_imgs = v_imgs.reshape(10000, 32, 32, 3)\n",
    "v_labels = np.float32(data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################### Convolution layer#############################################\n",
    "############################### Please implement the forward abd backward method in this class ############## \n",
    "class Conv:\n",
    "\n",
    "    def __init__(self,f,k,stride=1,pad=0):\n",
    "        edf.components.append(self)\n",
    "        self.f = f\n",
    "        self.k = k\n",
    "        pad = np.array(pad)\n",
    "        if pad.shape == ():\n",
    "            self.xpad = self.ypad = pad\n",
    "        else:\n",
    "            self.ypad = pad[0]\n",
    "            self.xpad = pad[1]\n",
    "            \n",
    "        self.stride=stride\n",
    "        self.grad = None if f.grad is None and k.grad is None else edf.DT(0) \n",
    "\n",
    "    ####################### Please implement this function####################### \n",
    "    def forward(self):\n",
    "        inshape = self.k.shape\n",
    "        if self.value is None:\n",
    "            owsize = np.ceil((inshape[1] - self.f.shape[0] + 1) / self.stride)\n",
    "            ohsize = np.ceil((inshape[2] - self.f.shape[0] + 1) / self.stride)\n",
    "            self.value = np.ndarray([inshape[0], owsize, ohsize, inshape[3]], \n",
    "                                    np.dtype(np.float64))\n",
    "        \n",
    "        # padding\n",
    "        padded = np.ndarray([inshape[0], inshape[1] + 2 * self.pad, inshape[2] + 2 * self.pad, inshape[3]])\n",
    "        padded.fill(0)\n",
    "        padded[:, self.pad: inshape[1] + self.pad , self.pad : inshape[2] + self.pad ,:] = self.k.value\n",
    "        yshape = self.value.shape\n",
    "        ksize = self.f.shape[0]\n",
    "        for bi in range(yshape[0]):\n",
    "            for ci in range(yshape[3]):\n",
    "                for wi in range(yshape[1]):\n",
    "                    for hi in range(yshape[2]):\n",
    "                        self.value[bi, wi, hi, ci] = np.multiply(padded[bi, \n",
    "                                                                        self.stride * wi : self.stride * wi + ksize,\n",
    "                                                                        self.stride * hi : self.stride * hi + ksize,\n",
    "                                                                        :], \n",
    "                                                                 self.f[:,:,:,ci])\n",
    "                                                        .sum()\n",
    "                \n",
    "    ####################### Please implement this function#######################         \n",
    "    def backward(self):\n",
    "        if self.k.grad is None or self.f.grad is None:\n",
    "            return\n",
    "        fgrad = np.ndarray(self.f.value.shape, np.dtype(np.float64))\n",
    "        fgrad.fill(0)\n",
    "        kgrad = np.ndarray(self.k.value.shape, np.dtype(np.float64))\n",
    "        kgrad.fill(0)\n",
    "        \n",
    "        kshape = self.k.value.shape\n",
    "        fshape = self.f.value.shape\n",
    "        yshape = self.value.shape\n",
    "        for bi in range(kshape[0]):\n",
    "            for ci in range(kshape[3]):\n",
    "                for c2i in range(kshape[3]):\n",
    "                    for wi in range(yshape[1]):\n",
    "                        for hi in range(yshape[2]):\n",
    "                            for wki in range(fshape[0]):\n",
    "                                for hki in range(fshape[0]):\n",
    "                                    xwi = self.stride * wi + wki\n",
    "                                    xhi = self.stride * hi + whi\n",
    "                                    kgrad[bi, xwi, xhi, ci] += self.grad[bi, wi, hi, c2i] * self.f.value[wki, hki, c2i, ci]\n",
    "                                    fgrad[wki, hki, c2i, ci] += self.grad[bi, wi, hi, c2i] * self.k.value[bi, xwi, xhi, ci]\n",
    "        self.f.grad += fgrad\n",
    "        self.k.grad += kgrad\n",
    "\n",
    "########################################### MaxPool layer#############################################\n",
    "############################### Please implement the forward abd backward method in this class ##############             \n",
    "class MaxPool:\n",
    "    def __init__(self,x,ksz=2,stride=None):\n",
    "        edf.components.append(self)\n",
    "        self.x = x\n",
    "        self.ksz=ksz\n",
    "        if stride is None:\n",
    "            self.stride=ksz\n",
    "        else:\n",
    "            self.stride=stride\n",
    "        self.grad = None if x.grad is None else edf.DT(0)\n",
    "\n",
    "    ####################### Please implement this function#######################     \n",
    "    def forward(self):\n",
    "        if self.value is None:\n",
    "            xshape = self.x.value.shape\n",
    "            self.value = np.ndarray([xshape[0], \n",
    "                                     xshape[1] / self.stride, \n",
    "                                     xshape[2] / self.stride, \n",
    "                                     xshape[3]], \n",
    "                                    self.x.value.dtype)\n",
    "            self.xmaxs = {}\n",
    "        sshape = self.value.shape\n",
    "        for bi in range(sshape[0]):\n",
    "            for ci in range(sshape[3]):\n",
    "                for wi in range(sshape[1]):\n",
    "                    for hi in range(sshape[2]):\n",
    "                        xmax = np.NINF\n",
    "                        for swi in range(self.stride):\n",
    "                            for shi in range(self.stride):\n",
    "                                # Record the maximal value and its location\n",
    "                                xvalue = self.x.value[bi, wi * self.stride + swi, \n",
    "                                                      hi * self.stride + shi, ci]\n",
    "                                record = (bi, wi * self.stride + swi, hi * self.stride + shi, ci)\n",
    "                                if xvalue > xmax:\n",
    "                                    xmax = xvalue\n",
    "                                    self.xmaxs[(bi,wi,hi,ci)] = [record]\n",
    "                                elif xvalue == xmax:\n",
    "                                    self.xmaxs[(bi,wi,hi,ci)].append(record)\n",
    "                        self.value[bi, wi, hi, ci] = xmax\n",
    "                        \n",
    "    ####################### Please implement this function#######################             \n",
    "    def backward(self):\n",
    "        if self.x.grad is None:\n",
    "            return\n",
    "        grad = np.ndarray(self.x.value.shape, np.dtype(np.float64))\n",
    "        # for each grad, prop only to the max inputs\n",
    "        sshape = self.value.shape\n",
    "        for bi in range(sshape[0]):\n",
    "            for ci in range(sshape[3]):\n",
    "                for wi in range(sshape[1]):\n",
    "                    for hi in range(sshape[2]):\n",
    "                        gval = self.grad[bi,wi,hi,ci]\n",
    "                        for xmaxp in self.xmaxs[(bi,wi,hi,ci)]:\n",
    "                            grad[xmaxp[0],xmaxp[1],xmaxp[2],xmaxp[3]] = gval\n",
    "        self.x.grad += grad\n",
    "########################################### AvePool layer#############################################\n",
    "############################### Please implement the forward abd backward method in this class ##############                             \n",
    "class AvePool:\n",
    "    def __init__(self,x,ksz=2,stride=None):\n",
    "        edf.components.append(self)\n",
    "        self.x = x\n",
    "        self.ksz=ksz\n",
    "        if stride is None:\n",
    "            self.stride=ksz\n",
    "        else:\n",
    "            self.stride=stride\n",
    "        self.grad = None if x.grad is None else edf.DT(0)\n",
    "        \n",
    "    ####################### Please implement this function#######################   \n",
    "    def forward(self):\n",
    "        if self.value is None:\n",
    "            xshape = self.x.value.shape\n",
    "            self.value = np.ndarray([xshape[0], \n",
    "                                     xshape[1]/self.stride,  \n",
    "                                     xshape[2]/self.stride, \n",
    "                                     xshape[3]], \n",
    "                                    self.x.value.dtype)\n",
    "        sshape = self.value.shape\n",
    "        for bi in range(sshape[0]):\n",
    "            for ci in range(sshape[3]):\n",
    "                for wi in range(sshape[1]):\n",
    "                    for hi in range(sshape[2]):\n",
    "                        xsum = edf.DT(0)\n",
    "                        for swi in range(self.stride):\n",
    "                            for shi in range(self.stride):\n",
    "                                xsum += self.x.value[bi, wi * self.stride + swi, \n",
    "                                                      hi * self.stride + shi, ci]\n",
    "                        xavg = xsum / (np.square(self.stride))\n",
    "                        self.value[bi, wi, hi, ci] = xavg\n",
    "    ####################### Please implement this function#######################    \n",
    "    def backward(self):\n",
    "        if self.x.grad is None:\n",
    "            return\n",
    "        grad = np.ndarray(self.x.value.shape, np.dtype(np.float64))\n",
    "        sshape = self.value.shape\n",
    "        for bi in range(sshape[0]):\n",
    "            for ci in range(sshape[3]):\n",
    "                for wi in range(sshape[1]):\n",
    "                    for hi in range(sshape[2]):\n",
    "                        gval = self.grad[bi,wi,hi,ci]\n",
    "                        for wsi in range(self.stride):\n",
    "                            for hsi in range(self.stride):\n",
    "                                grad[bi, wi * self.stride + wsi, hi * self.stride + hsi, ci] = gval\n",
    "        \n",
    "        self.x.grad += grad / np.square(self.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for repeatability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Inputs\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "\n",
    "prev_channel = 3 # RGB channel \n",
    "########################## Simple Convolution Nerual Network Model for Cifar 10 ##################################\n",
    "##################################################################################################################\n",
    "# please implement your main cnn model here, as described by the homework, you can mimic the previous code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the standard classification layer, which you don't need to modify\n",
    "pred = edf.SoftMax(pred)\n",
    "loss = edf.Mean(edf.LogLoss(edf.Aref(pred,lab)))\n",
    "acc = edf.Accuracy(pred,lab)\n",
    "\n",
    "\n",
    "################################################################################################################## \n",
    "# evaluation bucket\n",
    "bucket = 100\n",
    "def eval_train():    \n",
    "    \n",
    "    # we only choose 1/5 of the train images for evaluation since evaluation the whole images is time consuming\n",
    "    eval_imgs = t_imgs[::5]\n",
    "    eval_labels = t_labels[::5]\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for seq in range(bucket):\n",
    "        inp.set(eval_imgs[seq::bucket])\n",
    "        lab.set(eval_labels[seq::bucket])\n",
    "        edf.Forward()\n",
    "        avg_acc += acc.value\n",
    "        avg_loss += loss.value\n",
    "    \n",
    "    return avg_acc/bucket, avg_loss/bucket\n",
    "        \n",
    "def eval_test():\n",
    "    \n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    for seq in range(bucket):\n",
    "        inp.set(v_imgs[seq::bucket])\n",
    "        lab.set(v_labels[seq::bucket])\n",
    "        edf.Forward()\n",
    "        avg_acc += acc.value\n",
    "        avg_loss += loss.value\n",
    "    \n",
    "    return avg_acc/bucket, avg_loss/bucket\n",
    "\n",
    "# initial accuracy \n",
    "random_acc, random_loss = eval_test()\n",
    "print(\"Random test loss = %.4f, accuracy = %.4f\" % (random_loss, random_acc))\n",
    "\n",
    "\n",
    "################################################# train loop ######################################################\n",
    "ep = 0\n",
    "epoch = 10\n",
    "batch = 100\n",
    "train_loss = []; train_acc = []; test_loss =[]; test_acc = []\n",
    "stime = time()\n",
    "batches = range(0, len(t_labels), batch)\n",
    "\n",
    "while ep < epoch:\n",
    "\n",
    "    # randon shuffle the train data in each epoch\n",
    "    perm = np.random.permutation(len(t_labels))\n",
    "\n",
    "    for k in batches:\n",
    "        inp.set(t_imgs[perm[k:k+batch]])\n",
    "        lab.set(t_labels[perm[k:k+batch]])\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.Adam()\n",
    "        \n",
    "    # evaluate on trainset\n",
    "    t_acc, t_loss = eval_train()\n",
    "    print(\"Epoch %d: train loss = %.4f [%.3f secs]\" % (ep, t_loss,time()-stime))\n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "\n",
    "    # evaluate on testset\n",
    "    v_acc, v_loss = eval_test()\n",
    "    print(\"test accuracy = %.4f\" % v_acc)\n",
    "    test_loss.append(v_loss)\n",
    "    test_acc.append(v_acc)\n",
    "    stime = time()\n",
    "    ep += 1      \n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss)), test_loss, color='red')\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(len(test_acc)), test_acc, color='red')\n",
    "plt.plot(np.arange(len(train_acc)), train_acc, color='blue')\n",
    "plt.legend(['test acc', 'train acc'], loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
