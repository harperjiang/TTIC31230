{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt', False)\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt', False)\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 49.97699 Avg loss = 3.94506\n",
      "Initial generated sentence \n",
      "the agreements bring6d'\\z{33jjc1wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_GRU.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "\n",
    "np.random.seed(0)\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "Wz = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "Wr = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "W = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wz, Wr, W, V])\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "def GRUCell(xt, h):\n",
    "    \n",
    "    z = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wz))\n",
    "    r = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wr))\n",
    "    h_hat = edf.Tanh(edf.VDot(edf.ConCat(xt, edf.Mul(r, h)), W))\n",
    "    h_next = edf.Add(edf.Mul(z, h_hat), edf.Mul(h, edf.Add(edf.Value(1), edf.Mul(z, edf.Value(-1)))))\n",
    "    \n",
    "    return h_next\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next \n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "           \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx  \n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
