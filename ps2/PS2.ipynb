{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindata = './mnist_data/train.npz'\n",
    "valdata = './mnist_data/test.npz'\n",
    "\n",
    "data = np.load(traindata)\n",
    "t_imgs = np.float32(data['imgs'])/255.\n",
    "t_labels = np.float32(data['labels'])\n",
    "\n",
    "data = np.load(valdata)\n",
    "v_imgs = np.float32(data['imgs'])/255.\n",
    "v_labels = np.float32(data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################# Demonstration that SGD works for min-batch setting ############################## \n",
    "\n",
    "\n",
    "######################################### please modify this cell to finish the problem 2.a #######################\n",
    "# for repeatability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "W1 = edf.Param(edf.xavier((28*28,128)))\n",
    "B1 = edf.Param(np.zeros((128)))\n",
    "W2 = edf.Param(edf.xavier((128,10)))\n",
    "B2 = edf.Param(np.zeros((10)))\n",
    "\n",
    "# models\n",
    "hidden = edf.RELU(edf.Add(edf.VDot(inp,W1),B1))\n",
    "pred = edf.SoftMax(edf.Add(edf.VDot(hidden,W2),B2))\n",
    "loss = edf.LogLoss(edf.Aref(pred, lab))\n",
    "acc = edf.Accuracy(pred,lab)\n",
    "\n",
    "# batch size, please try 10, 50 and 100. For each run, you might need to reloading the kernel (edf.py) \n",
    "# to clear the history information\n",
    "batch = 10\n",
    "# learning rate eta, measured by per-batch unit. If you change this batch size, you might also change eta \n",
    "# accoridng to the equation given in the homework.\n",
    "eta = 0.37\n",
    "# eta = 0.0056 * batch + 0.0659\n",
    "# evaluate the random performance\n",
    "def eval(imgs, labels):\n",
    "    \n",
    "    batches = range(0, len(labels), batch)\n",
    "    objective = 0\n",
    "    accuracy = 0\n",
    "    for k in batches:\n",
    "        inp.set(t_imgs[k:k+batch])\n",
    "        lab.set(t_labels[k:k+batch])\n",
    "        edf.Forward()\n",
    "        objective += np.mean(loss.value)\n",
    "        accuracy += acc.value\n",
    "    \n",
    "    return accuracy/len(batches), objective/len(batches)\n",
    "\n",
    "\n",
    "accuracy, objective = eval(t_imgs, t_labels)\n",
    "print(\"Random accuracy = %.4f\" % accuracy)\n",
    "\n",
    "# train loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "ep = 0\n",
    "stime = time()\n",
    "epoch = 10\n",
    "batches = range(0, len(t_labels), batch)\n",
    "\n",
    "while ep < epoch:\n",
    "\n",
    "    # randon shuffle the train data in each epoch\n",
    "    perm = np.random.permutation(len(t_labels))\n",
    "    for k in batches:    \n",
    "        inp.set(t_imgs[perm[k:k+batch]])\n",
    "        lab.set(t_labels[perm[k:k+batch]])\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.SGD(eta)\n",
    "\n",
    "    # evaluate on trainset\n",
    "    t_acc, t_loss = eval(t_imgs, t_labels)\n",
    "    print(\"Epoch %d: train loss = %.4f [%.4f secs]\" % (ep, t_loss,time()-stime))\n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "\n",
    "    # evaluate on testset\n",
    "    v_acc, v_loss = eval(v_imgs, v_labels)\n",
    "    print(\"test accuracy=%.5f\" % v_acc)\n",
    "    test_loss.append(v_loss)\n",
    "    test_acc.append(v_acc)\n",
    "    stime = time()\n",
    "    ep += 1\n",
    "\n",
    "# plot\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss)), test_loss, color='red')\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(len(test_acc)), test_acc, color='red')\n",
    "plt.plot(np.arange(len(train_acc)), train_acc, color='blue')\n",
    "plt.legend(['test acc', 'train acc'], loc='lower right')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################### please modify this cell to finish the problem 2.b #########################################\n",
    "\n",
    "\n",
    "# Optimization functions with Momentum algorithm, eta is learning rate and mom is momentum rate\n",
    "# please implement this function\n",
    "def Momentum(eta, mom = 0.55):\n",
    "    # Initialize the \"grad_hist\" variable to memorize the history of gradient\n",
    "    if 'grad_hist' not in edf.params[0].__dict__.keys():\n",
    "        for p in edf.params:\n",
    "            p.grad_hist = edf.DT(0)\n",
    "    # please add code here to finish the function\n",
    "    for p in edf.params:\n",
    "        p.grad_hist = mom * p.grad_hist + (1 - mom)* p.grad\n",
    "        p.value -= eta * p.grad_hist\n",
    "        p.grad = edf.DT(0)\n",
    "    \n",
    "    \n",
    "# for repeatability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "W1 = edf.Param(edf.xavier((28*28,128)))\n",
    "B1 = edf.Param(np.zeros((128)))\n",
    "W2 = edf.Param(edf.xavier((128,10)))\n",
    "B2 = edf.Param(np.zeros((10)))\n",
    "\n",
    "# models\n",
    "hidden = edf.RELU(edf.Add(edf.VDot(inp,W1),B1))\n",
    "pred = edf.SoftMax(edf.Add(edf.VDot(hidden,W2),B2))\n",
    "loss = edf.LogLoss(edf.Aref(pred,lab))\n",
    "acc = edf.Accuracy(pred,lab)\n",
    "\n",
    "# batch size, please also try 10 and 100\n",
    "batch = 100\n",
    "# learning rate eta, measured by per-batch. If you change this batch size, you might also change eta \n",
    "# accoridng to the equation given in the homework.\n",
    "eta = 0.37\n",
    "#eta = 0.0056 * batch + 0.0659\n",
    "\n",
    "# evaluate the random performance\n",
    "def eval(imgs, labels):\n",
    "    \n",
    "    batches = range(0, len(labels), batch)\n",
    "    objective = 0\n",
    "    accuracy = 0\n",
    "    for k in batches:\n",
    "        inp.set(t_imgs[k:k+batch])\n",
    "        lab.set(t_labels[k:k+batch])\n",
    "        edf.Forward()\n",
    "        objective += np.mean(loss.value)\n",
    "        accuracy += acc.value\n",
    "    \n",
    "    return accuracy/len(batches), objective/len(batches)\n",
    "\n",
    "\n",
    "accuracy, objective = eval(t_imgs, t_labels)\n",
    "print(\"Random accuracy = %.4f\" % accuracy)\n",
    "\n",
    "# train loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "ep = 0\n",
    "stime = time()\n",
    "epoch = 10\n",
    "batches = range(0, len(t_labels), batch)\n",
    "\n",
    "while ep < epoch:\n",
    "\n",
    "    # randon shuffle the train data in each epoch\n",
    "    perm = np.random.permutation(len(t_labels))\n",
    "    for k in batches:    \n",
    "        inp.set(t_imgs[perm[k:k+batch]])\n",
    "        lab.set(t_labels[perm[k:k+batch]])\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        \n",
    "        # here, we use Momentum to optimize as in problem 2.b\n",
    "        Momentum(eta)\n",
    "    # evaluate on trainset\n",
    "    t_acc, t_loss = eval(t_imgs, t_labels)\n",
    "    print(\"Epoch %d: train loss = %.4f [%.3f secs]\" % (ep, t_loss,time()-stime))\n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "\n",
    "    # evaluate on testset\n",
    "    v_acc, v_loss = eval(v_imgs, v_labels)\n",
    "    print(\"test accuracy=%.4f\" % v_acc)\n",
    "    test_loss.append(v_loss)\n",
    "    test_acc.append(v_acc)\n",
    "    stime = time()\n",
    "    ep += 1\n",
    "\n",
    "# plot\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss)), test_loss, color='red')\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(len(test_acc)), test_acc, color='red')\n",
    "plt.plot(np.arange(len(train_acc)), train_acc, color='blue')\n",
    "plt.legend(['test acc', 'train acc'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################### please modify this cell to finish the problem 2.c #########################################\n",
    "\n",
    "# Optimization functions with Adam optimization algorithm.\n",
    "# For details, please see: https://arxiv.org/abs/1412.6980, \n",
    "# Please implement this function\n",
    "\n",
    "# some constant used in Adam\n",
    "_a_b1t=edf.DT(1.0)\n",
    "_a_b2t=edf.DT(1.0)\n",
    "\n",
    "def Adam(eta=0.001, b1 = 0.9, b2 = 0.999, ep=1e-8):\n",
    "    \n",
    "    global _a_b1t\n",
    "    global _a_b2t\n",
    "\n",
    "    # Initialize the \"grad_hist\" variable to memorize the history of gradient\n",
    "    # # Initialize the \"grad_h2\" variable to memorize the history of gradient variance\n",
    "    if 'grad_hist' not in edf.params[0].__dict__.keys():\n",
    "        for p in edf.params:\n",
    "            p.grad_hist = edf.DT(0)\n",
    "            p.grad_h2 = edf.DT(0)\n",
    "\n",
    "    # please finish this function\n",
    "    for p in edf.params:\n",
    "        p.grad_hist = b1 * p.grad_hist + (1 - b1) * p.grad\n",
    "        p.grad_h2 = b2 * p.grad_h2 + (1 - b2) * np.square(p.grad)\n",
    "        hist_hat = p.grad_hist / _a_b1t\n",
    "        h2_hat = p.grad_h2 / _a_b2t\n",
    "        p.value -= eta * hist_hat / (np.sqrt(h2_hat) + ep)\n",
    "        p.grad = edf.DT(0)\n",
    "    \n",
    "    \n",
    "# for repeatability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "W1 = edf.Param(edf.xavier((28*28,128)))\n",
    "B1 = edf.Param(np.zeros((128)))\n",
    "W2 = edf.Param(edf.xavier((128,10)))\n",
    "B2 = edf.Param(np.zeros((10)))\n",
    "\n",
    "# models\n",
    "hidden = edf.RELU(edf.Add(edf.VDot(inp,W1),B1))\n",
    "pred = edf.SoftMax(edf.Add(edf.VDot(hidden,W2),B2))\n",
    "loss = edf.LogLoss(edf.Aref(pred,lab))\n",
    "acc = edf.Accuracy(pred,lab)\n",
    "\n",
    "\n",
    "# batch size, try set to 10, 50, 100\n",
    "batch = 50\n",
    "# eta is the learning rate and measured by per-batch unit, please tune it a little bit under different batch size.\n",
    "eta = 0.0017\n",
    "\n",
    "\n",
    "# evaluate the random performance\n",
    "def eval(imgs, labels):\n",
    "    \n",
    "    batches = range(0, len(labels), batch)\n",
    "    objective = 0\n",
    "    accuracy = 0\n",
    "    for k in batches:\n",
    "        inp.set(t_imgs[k:k+batch])\n",
    "        lab.set(t_labels[k:k+batch])\n",
    "        edf.Forward()\n",
    "        objective += np.mean(loss.value)\n",
    "        accuracy += acc.value\n",
    "    \n",
    "    return accuracy/len(batches), objective/len(batches)\n",
    "\n",
    "\n",
    "accuracy, objective = eval(t_imgs, t_labels)\n",
    "print(\"Random accuracy = %.4f\" % accuracy)\n",
    "\n",
    "# train loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "ep = 0\n",
    "stime = time()\n",
    "epoch = 10\n",
    "batches = range(0, len(t_labels), batch)\n",
    "\n",
    "while ep < epoch:\n",
    "\n",
    "    # randon shuffle the train data in each epoch\n",
    "    perm = np.random.permutation(len(t_labels))\n",
    "    for k in batches:    \n",
    "        inp.set(t_imgs[perm[k:k+batch]])\n",
    "        lab.set(t_labels[perm[k:k+batch]])\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        \n",
    "        # here, we use Adam algorithm to optimize as in problem 2.c\n",
    "        Adam(eta)\n",
    "\n",
    "    # evaluate on trainset\n",
    "    t_acc, t_loss = eval(t_imgs, t_labels)\n",
    "    print(\"Epoch %d: train loss = %.4f [%.3f secs]\" % (ep, t_loss,time()-stime))\n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "\n",
    "    # evaluate on testset\n",
    "    v_acc, v_loss = eval(v_imgs, v_labels)\n",
    "    print(\"test accuracy=%.4f\" % v_acc)\n",
    "    test_loss.append(v_loss)\n",
    "    test_acc.append(v_acc)\n",
    "    stime = time()\n",
    "    ep += 1\n",
    "\n",
    "# plot\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss)), test_loss, color='red')\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(len(test_acc)), test_acc, color='red')\n",
    "plt.plot(np.arange(len(train_acc)), train_acc, color='blue')\n",
    "plt.legend(['test acc', 'train acc'], loc='lower right')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
